\documentclass{article}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[english,activeacute]{babel}
\usepackage{mathtools}
\usepackage{biblatex}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage[]{algorithm2e}

%\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\title{User Behavior Analysis in Campus Area Networks through Kohonen Self Organizing Feature Maps}

\author{Nelson Victor Cruz Hern\'andez }

\date{June 2017}

\begin{document}

\maketitle






\section{Introduction} % level 1




\subsection{Background} % level 2
En base al reporte de seguridad de Cisco publicado en 2015 [1] los usuarios y los equipos de Tecnologías de información se han vuelto clave en la seguridad informática dado que estos comprometen la seguridad de forma inconsciente a través del uso indiscriminado de los navegadores y la participación en campañas de publicidad [1], principales medios para la distribución de malware.

Las técnicas actuales de seguridad en redes de computadoras trabajan principalmente en la frontera de la red, buscando prevenir los ataques que son ejecutados desde el exterior descuidando en mayor forma las amenazas que se originan en el interior. 

Se han desarrollados diversos trabajos buscando prevenir y contener los ataques por parte de usuarios internos con diferentes propuestas como son; técnicas de Machine Learning y Sistemas de Detección de Intrusos (IDS por sus siglas en inglés) [2] [3], algoritmos de clustering [2, 4, 5], generación de infraestructuras de cloud computing y minería de datos [4]. 
Dentro de las propuestas de nuevas técnicas de IDS existe la problemática de los altos índices de falsos positivos, una tasa de detección que varía en base al tipo de ataque, y una dependencia completa a la creación de firmas que muchas veces pueden nos ser optimas en el ambiente de producción o sus tiempo de creación es demasiado largo, siendo las principales razones de esto la extensa cantidad de información que debe ser analizada, el poco tiempo que se tiene para su análisis, el uso incorrecto de las técnicas de procesamiento aplicadas o el tiempo que se debe invertir para la creación de firmas para el ambiente vulnerable.

Tian [3] propone que es posible hacer uso de selección de variables, para  reducir el conjunto de información a analizar lo que impacta directamente en el tiempo que se toma el análisis y es posible analizar de forma más detallada la información. Por su parte Kamarularin [2] explora la eficiencia de los procesos.

Por su lado Yamada [5] y Kamarularin [2] ejecutan diferentes algoritmos de machine learning cuyos resultados permitan conocer y comparar resultados en la detección de ataques y la retroalimentación para la generación de firmas de forma automatizada.

Es posible observar que existe un campo de estudio enfocado a los IDS, utilizando técnicas de machine learning con un alto nivel fiabilidad, por lo que su aplicación permitiría combatir de forma efectiva la brecha de seguridad en redes causada por los mismos usuarios




\subsection{Justification} % level 2
Current security network solutions neglect  the internal monitoring of users activity, as they assume that inner user interaction is secure and will not harm any asset of the network [10] However the lack of supervision and monitoring arise two new kind of network vulnerabilities: 1) an inner user attacks the network with full knowledge of what he is doing and 2) an inner user is a victim of an attacker, unconsciently causing attacks to the network or networks where it is has any activity. In both scenarios, the user is taking advantage of its unsupervised position.

In the first case, when the user is aware of its not authorized actions, its possible to find two different profiles: 1) basic users and 2) intermediate-advance users.
Basic malicious users are forced to explode known system vulnerabilities, depending on missing security patches in the operating systems and to use code (snippets and scripts) written by experienced users, as they have a lack of technical formation, making its network behavior similar to a \textit{scriptkiddie}. On the other hand, intermediate-advance users may use new specialized software or tools to break into the network and damage valuable information.

In the case where the host is being used by an attacker, both previously behaviors could be detected over the network.

As normal inner users and anomalous users interact in the same network, characterization of both behaviors could provide the required information to detect anomalous users.



\subsection{Problem} % level 2
Identifying network attacks has been addressed in two different ways: 1) [CONCEPT1], known attack firms are provided to security network devices such as firewalls, antivirus or intrusion detection systems to check if the current network traffic matches any of them and 2) [CONCEPT2], use the artificial intelligence or machine learning algorithms to detect anomalous behavior.

In [CONCEPT1] network devices, are able to detect anomalous behavior that completely matches any of the firms in the known attacks database (KAD), missing by far corner cases such as variations, or improvements of the same attack. Besides this, novel attacks are completely ignored as they do not match any firm in KAD. Both scenarios make keeping up with security updates is tedious, time consuming and has a high economic impact.

In [CONCEPT2], in order to avoid firms classification which implies spending time and human resources, firms are obtained from general datasets such as KDCUPP99 or ANOTHERDATASET [REFERENCES, XX, XX, XX], causing a big rate of false-positives, making the system not as reliable as it should be.

Attack firms usage is not a viable meanwhile its process of creation is too long, experienced human resources are limited or expensive and updates are not always available due they not exist or are very expensive.



\subsection{Hypothesis} % level 2
As we form our own criteria, our environment, actions and thoughts start to model how we behave and respond to different stimulus in our day to day situations, in the same way, with the usage of internet in every aspect of our life, a network behavior pattern could be obtained from our digital interaction.

In this work, we state that an user has a defined behavior in a network. This behavior could be obtained by the processing user's network activity into an user network behavior pattern (UNBP), allowing us to identify an specific user among others.




\subsection{Objectives} % level 2
\subsubsection{General Objectives} % level 3
Prevenir problemas de seguridad interna en campus area networks (CAN) a través del perfilado de usuarios de la red, mediante técnicas de clustering y machine learning, haciendo posible la detección de ataques de usuarios internos en un periodo corto de tiempo, y la reducción de falsos positivos.

\subsubsection{Particular Objectives} % level 3
Para poder lograr el objetivo general es necesario desarrollar las siguientes fases:
Implementar un algoritmo de clustering cuyo resultado nos permita identificar grupos de usuarios con comportamientos similares.
Hacer uso de los clusters obtenidos para definir patrones de comportamiento normal dentro de la red, con el fin de establecer el perfil de un usuario normal y recurrente de la red.
Identificar un usuario anómalo, basado en la comparación del clúster y el perfil de un usuario normal en la red.
Clasificar el nivel de conocimiento de un usuario malicioso en: 1) básico, aquel usuario cuya actividad en la red sea para investigación referente a seguridad en redes de computadoras, descarga de scripts y ejecución de estos, o manejo de sistemas operativos enfocados a pen testing 2) intermedio o 3) avanzado
Retroalimentar el set de clusters existentes con el comportamiento de algún usuario anómalo, con el objetivo de actualizar los clusters existentes con la nueva información o crear un nuevo clúster con un comportamiento propio definido.






\section{State of the Art} % level 1
Chaudhari y Prasad [4] establecen los principales problemas que se presentan actualmente en los IDS que son: 1) El origen de los datos con los que un IDS trabaja y el recorrido que estos deben hacer previo a su análisis, lo que causa que en repetidas ocasiones se haga una mala interpretación de los datos debido a la falta de contexto de la información; 2) el exceso de uso de recursos extras para su funcionamiento continuo; y, 3) la dependencia de los IDS hacia otros módulos de que pueden ser susceptibles a ataques y por lo tanto dejar temporalmente o de forma definitiva inhabilitado el sistema.
Autores como Kamarularin [2], Chaudhari [4], Xu [6] y Muda[7], abordan diferentes técnicas de machine learning, mientras Pilabutr [8], Tian [3] y Al-Jarrah [9] se enfocan en la selección de variables, adicionalmente Yamada [5] nos presenta metodologías para la generación de datasets de entrenamiento y pruebas.




\subsection{Machine Learning Algorithms and Computer Security} % level 2
Kamarularin [2] propone evaluar el desempeño del algoritmo de árboles de decisión y compararlo con los algoritmos de redes neuronales y SVM con el objetivo de evaluar que algoritmo de estos es  más efectivo para la detección de ataques. Utilizando para esto cuatro criterios a evaluar: 1) la precisión en la detección; 2) la tasa de detección; 3) los falsos positivos que presento cada uno de los algoritmos; 4) la precisión para clasificar los ataques en su correspondiente categoría: Probe, Denial of Service (DoS), User to root (U2R), Remote to local (R2L). Basado en esto el algoritmo que mejor se desempeña es Arboles de Decisión, mostrando una tase de detección del 98.55%. 
Xu, Wang y Gu [6] proponen una nueva forma de hacer un perfilamiento del tráfico de red identificando y analizando clusters de hosts y aplicaciones que tienen un comportamiento similar, reduciendo significativamente el costo del análisis del tráfico. Dicho análisis se logra en cuatros fases: 
1) Creación de grafos bipartitos para modelar el tráfico de red y representar patrones de comunicación entre hosts origen y hosts destinos.
2) Generación de one-mode projections a partir de los grafos bipartitos, con el fin de descubrir de forma eficiente relaciones ocultas entre nodos con los mismos vértices y descubrir comportamientos similares.
3) Construcción de matrices de similitud basados en las proyecciones previamente obtenidas, donde la caracterización de estas, está basada en el número de host destinos, y hosts orígenes comparten.
4) Aplicación de un algoritmo de clustering, para agrupar comportamientos similares basados en la caracterización que poseen las matrices, donde cada clúster consiste en host orígenes que se comunican con hosts destinos con los mismos prefijos de red, servidores, o inclusive clientes. 
Con el análisis anterior es posible encontrar un número finito de clusters con un comportamiento definido, que es más fácil de analizar comparado a un análisis de tráfico de hosts individuales, además de que permite encontrar el comportamiento de un grupo de hosts en el mismo prefijo de red. Xu et. al [6] Logran mostrar que mediante su técnica de perfilamiento es posible crear clusters con comportamientos e intercambio de información diferentes que son consistentes en el tiempo.
Debido a la alta tasa de falsos positivos que los algoritmos de machine learning arrojan, Muda [8] propone crear un método hibrido de detección de intrusos basado en K-Means y la técnica de clasificación OneR, dicho algoritmo funciona a través de la agrupación de datos, basado en el comportamiento que presentan (comportamiento normal y comportamiento malicioso) a través del algoritmo de clustering K-means, una vez agrupados, el algoritmo de clasificación OneR permite identificar las diferentes clases de ataques que están presentes. El desempeño del algoritmo es comparado con otros algoritmos: k-NN, Hierarchical Clustering + SVM y ESC-IDS, mostrando una reducción en la tasa de falsos positivos del 4.56% respecto a los demás algoritmos. 
Chaudhari  [4] y Tian W. [3] presentan la propuesta de hacer uso del algoritmo de Particle Swarm Optimization (PSO), una técnica heurística de optimización cuya funcionalidad está basada en el comportamiento social que tienen las aves cuando vuelan o los peces cuando necesitan protección de depredadores más grandes. En este algoritmo cada partícula o elemento intenta buscar la mejor posición basado en la posición de sus compañeros alrededor. Chaudhari y Tian W. aprovechan las características del algoritmo de auto aprendizaje y rápida convergencia para aprender patrones típicos de un ataque a la red. 



\subsection{Profiling  and User classification} % level 2






\section{Theorical framework} % level 1




\subsection{Proxy} % level 2




\subsection{Machine Learning algorithms}	 % level 2


\subsubsection{Learning methods} % level 3

\paragraph{Supervised Training Methods} % level 4
Obtain the information from "Artificial Neural Networks An introduction Kevin L. Priddy and Paul E. Keller" Chapter 2.1

\paragraph{Unsupervised Training Methods} % level 4
Obtain the information from "Artificial Neural Networks An introduction Kevin L. Priddy and Paul E. Keller" Chapter 2.3

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \subsubsection{Gaussian function} % level 3


\subsection{Self-organizing Maps} % level 2
The Self-Organizing Map algorithm performs a nonlinear, ordered, smooth mapping of high-dimensional input data manifolds onto the elements of a regular, low-dimensional array [25]. The algorithm converts non-linear statistical relationships between data elements in a high-dimensional space into geometrical relationships between elements in a two-dimensional map (lattice), called the Self-Organizing Map (SOM)[1]. A SOM can then be used to visualize the clusters, of an input space. Each element at SOM is a neuron, and is a representation of a multidimensional vector with a cartographic position denoted with x and y. If elements in the input space are characterized using $k$ parameters and represented by $k$-dimensional vectors, each neuron in the SOM lattice is also specified as $k$-dimensional vector.


\subsubsection{Learning} % level 3
In the learning or training phase, the neurons in SOM algorithm try to model the input space. 
SOM algorithm differ from other artificial neuronal networks as they apply competitive learning and use a cooperative schema. 

Opposed to error-correction learning, in which each element $s_n$ in the training dataset $S$ is evaluated by the neuronal network connections $W(w_1, w_2,...,w_n)$ and the result $r$ is compared against a predefined threshold to decide if the result matches the expected output and an adjustment to $W$ is needed, in competitive learning each element $e_n$ of the training data set $E$ is shown to every neuron $N(n_1,n_2,...,n_n)$ in the SOM lattice Each neuron $n_n$ calculates a response $h$ to the shown element, based on a preselected distance measure. The neuron that gives the best response is called the winning neuron or best matching unit (BMU).

Suitable distance measure should be stablished in order to find the BMU. Two common used distance measures are 1) dot-product measure and 2) euclidean distance.
For using dot-product measure, lattice neurons $N$ and training elements $E$ should be normalized. Normalization of a vector $V(v_1, v_2, v_3,...,v_n)$ is a process of transforming it's components into
$(\frac{v_1}{\sqrt{v_1^2+v_2^2+...+v_n^2}},
\frac{v_2}{\sqrt{v_1^2+v_2^2+...+v_n^2}},
...,
\frac{v_n}{\sqrt{v_1^2+v_2^2+...+v_n^2}}
)$
 so that the modules of the normalized vector is unity. The dot-product of the input vector is calculated against all the neurons in the lattice, where dot-product of two vectors $Y(x_1, x_2, x_3,..., x_n)$ and $Z(z_1, z_2, z_3,..., z_n)$ is defined by: 
  
\begin{equation}
X \cdot Z = (x_1 \cdot z_1 + x_2 \cdot z_2 + x_3 \cdot z_3 + ... + x_n \cdot z_n)
\end{equation}
using this measure means that BMU is the one that gives the maximum dot-product value.
 
 In the other hand, euclidean distance measure does not need vector normalization and the BMU is defined for the minimum obtained distance. For two vectors $Y(y_1,y_2,...,y_n)$ and $Z(z_1,z_2,...,z_n)$ euclidean distance is given by:
 
\begin{equation}
D = \sqrt{(z_1-y_1)^2 + (z_2-y_2)^2 + ... + (z_n-y_n)^2}
\end{equation}
once a BMU is obtained, it's $k$-dimensional values are adjusted so in the future it responds better to a similar input $e_n$.

As SOM algorithm is not just a classification algorithm but also a clustering algorithm, a way of maintaining the dimensional relationships between elements in the same geometrical area is to get their $k$-dimensional values updated at the same time as the BMU does. This update is known as a cooperative schema, and it only works on neurons that are in the vicinity of the BMU defined by a neighborhood function.

\paragraph{Neighborhood Function} % level 4

\paragraph{Learning Function} % level 4


\subsection{Redes de Computadoras} % level 2


\subsubsection{Local Area Network (LAN)} % level 3


\subsubsection{Campus Area Network(CAN)} % level 3


\subsubsection{Network topology} % level 3


\subsubsection{OSI Model} % level 3
The Open Systems Interconnection (OSI), is an ISO standard for worldwide communications, that defines networking in terms of a vertical stack of seven layers. Control is passed from one layer to the next.

Starting at the top layer, information is passed to the lower layer until it reaches the bottom, once the information reaches the bottom (the physical medium), the information make its way to the destination. Just after the information reaches its destination, it travels up each layer until it reaches the appropriate level for translation. During data transmission among the layers, relevant information of that layer is attached (encapsulation), binding this information allows each layer to communicate with its relevant layer at destination. Fig 1 illustrates the information flow.

	\begin{center}\includegraphics[scale=0.4]{osi} \end{center}
OSI layers are designed to communicate with only one layer above and below it, and are developed independently, allowing flexibility and modularity. As mentioned before there are seven layers in the OSI model 1) Physical, 2) Data Link, 3) Network, 4) Transport, 5) Session, 6) Presentation and 7) Application. A brief explanation of what is done, the process of communication, and the used protocols on each layer will be explained above.

\paragraph{Physical layer} % level 4
The physical layer describes the electric and optical signals used for communicating between two hosts, is concerned with the transmission and reception of the unstructured raw bit stream over the physical medium, and carries those signals to for all the higher layers.

\paragraph{Data Link layer} % level 4
This layer takes care of the verification that bit sequences are passed correctly between two nodes, If errors occur, this level request the retransmission of the corrupted sequence, providing the upper layers an error free transmission over the link. To do this, data link layer provides: a) link control, in which logical establishment and termination between nodes is done, b) frame control, which defines the transmission rate, establish the sequence of the received data, verifies data integrity and c) determines when the node is allowed to use the physical medium. Common protocols of this layer are: HDCL for point to point and point to multipoint connections, and IEEE 802 for local networks [7]. 

\paragraph{Network layer} % level 4
Network layer role is to deliver a packet form one logical address (in the case of an IP, an IP) to another. Router devices use the generated frames to send packets through the network, depending on several key field of the datagram header such as: the source and destination IP address, Time To Live counter (TTL), and the checksum [9].

\paragraph{Transport layer} % level 4
According to [4] In the Open Systems Interconnection (OSI) communications model, the Transport layer ensures the reliable arrival of messages and provides error checking mechanisms and data flow controls. The Transport layer provides services for both connection-mode transmissions and for connectionless-mode transmissions. Two transport protocols are used over the internet: TCP [5] and UDP [6]. TCP ensures the delivery of the packages due the implementation of re transmission mechanisms such as error checking and numbering for check the sequence of the packages, however UDP is more often used for network transmissions where the lost of any package is not critical.

\paragraph{Session layer} % level 4
Session layer facilitates the exchange of data between applications, by organizing and structuring the the processes among them. The basic unit of this layer is the protocol data unit (PDU) [7].

\paragraph{Presentation layer} % level 4
The presentation layer is in charge of formatting the data to be presented to the application layer. this layer acts as a translator, changing from a custom format used by the application to a common format used in the receiver station [7].

\paragraph{Application layer} % level 4
This layer is used as window to network services by users and application processes through network applications and application protocols. Some common protocols in this model are HTTP, FTP, DNS, and SMTP.

\subsubsection{Network Security} % level 3

\subsubsection{Intrusion Detection Systems} % level 3

\section{Methodological Development} % level 1

\subsection{Experiment context} % level 2
Experiment was carried out on a Campus Area Network (CAN) that has a 16-bit network and a Windows domain controller, using a HTTP proxy. Among campus applications web and remote apps are included.
Email service is provided by Microsoft Exchange Server which is hosted outside the campus network.
The target users were full-time professors who had a computer with a static IP address and a wireless access with a dynamic IP address.
Five full-time professors (hereafter denoted as users) were selected for the experiment. For each one, real usage traffic was captured for inside and outside campus activities during a two labor weeks, and then processed.






\subsection{Explanation} % level 2
In this work Self Organizing Maps algorithm is used to create an user pattern inside a Campus Area Network. Experiments are divided in three phases: 1) network data capture, 2) data processing and 3) pattern evaluation.

For network data capture phase, a set of raw packages is obtained for each user through tcpdump library, process is explained in [Parres, XX].

For data processing phase, each set of raw packages is arbitrary divided in build, train and evaluate sets.
Each set is processed to compress raw packages into chunks of a five minutes window $t$ represented by three metrics that involve communication protocol, origin and destination ip and total transmitted bytes.
From the obtained build data set, a fixed number of elements $n$ is randomly selected, this number defines the size of the Self Organizing Map lattice $(n x n)$.
For lattice training, a fixed number of elements $e$ is randomly selected from train data set. Self Organizing Map is considered to be fully trained, after a cycle of ten epochs, as a result of this phase an user network behavior pattern is obtained.

Evaluation phase is done by joining different user network behavior patterns in one lattice, similar to a blanket filled of patches in which each patch is represented by an user network behavior pattern, creating what we define as the organization pattern. An organization evaluation set is build by all the user evaluation sets that belong to each user that conforms the organizational pattern. Each element of the organization evaluation set is shown to the organization pattern, resulting best matching unit is compared against the original user of the shown element to the lattice. Correct match of the user attribute of the shown element and user attribute of the best matching verifies that the shown element is able to recognize it's original user among others. The complete process of an experiments is shown in Fig. 1.

	\begin{center}\includegraphics[scale=0.2]{fig-two} \end{center}

For each phase in the process one or more modules were build, each module's output is used as an input for the next phase corresponding module.


\subsubsection{Network data capture} % level 3
Network data capture phase duration was at least two labor weeks,  in which network traffic was captured from each user's computer.
Capture module was build using jNetPcap, which works as a client application that once installed enables to capture continuously user network data in an IP packet format and save it in files of twenty megabytes each, naming them with file's creation timestamp. Average size of complete raw captured traffic for each user is three gigabytes, involving more than four million packages. Each register contains the characterization of a network connection by eight parameters: origin IP, destination IP, used protocol, local used port, remote used port, total transmitted bytes in the package, timestamp and the connection-way which establishes if the package is coming from local to remote (out), or from remote to local (in).



\subsubsection{Data processing} % level 3
Data processing phase is divided in four stages: 1) Raw data partition, 2) Data sets creation, 3) Self Organizing Map algorithm implementation and 4) User network behavior pattern creation. Two modules were build for this phase, data set and train module. Data set module involves Raw data partition and Data set creation stages and Train module involves Self Organizing Map algorithm implementation and User network behavior pattern creation. Fig 2 shows how the data set module is working.




\paragraph{Raw data partition} % level 4
As explained in section 3.3 Self Organizing Map algorithm is a not supervised algorithm due different information is needed en each phase of the algorithm. Complete user raw captured data $\alpha$ is divided in three subsets: build package set $\beta$, train package set $\gamma$, and evaluation package set $\phi$. As data captured is divided in files containing continuous user network data, dividing the complete raw data set in subsets, enables having en each subset different days of user behavior. Complete raw data set is divided equally between each subset.

\begin{equation}
\alpha = \beta \cup \gamma \cup \phi
\end{equation}




\paragraph{Data set creation} % level 4
Using TCP package as the working unit is not possible due the great volume of packages, and time consuming for processing each one individually [Reference, XX].

Instead, package sets $[\beta,\gamma,\phi]$ are individually processed and turned into chunks. The obtained chunks are the the working units and conform the different data sets, Build data set $\beta_1$, the Train data set $\gamma_1$ and the Evaluation data set $\phi_1$ respectively.

	\begin{center}\includegraphics[scale=0.2]{fig-three} \end{center}
	
Each chunk $C$ is defined with the following characteristics: a) It is conformed by one or more packages $C_n(p_1, p_2,...,p_n)$ that have a continuity in their timestamp field, b) It has a fixed time window $w$ but does not have a fixed number of packages, c) The difference between $p_n$ timestamp and $p_1$ timestamp is less or equal to the fixed time window $w$ and d) The difference between start timestamps of consecutive chunks is a fixed time $h$.

The elements in a package set are processed one by one, as follows:

\begin{enumerate}
  \item Total packages in the package set is obtained and assigned to $n$, the index of the element that is being processed is assigned to $i$ and a global index that holds the package index in which the chunk $C_n$ start is assigned to $g$.
  \item First chunk $C_1$ start timestamp $t_1$ is set with the timestamp of $p_1$ and finish timestamp $t_2$ is calculated ($t_1 + w$).
  \item While $i$ < $n$ and $p_n$ timestamp is less than $t_2$, $p_n$ is added to the package chunk's collection and $i$ is assigned to the next element in the set, otherwise the $p_n$ is not added to the package chunk's collection, and the chunk is complete.
  \item $C_1$ is added to the chunks data set's collection.
  \item New start timestamp is calculated $t_1 + h$ and assigned to the new chunk $C_n$.
\end{enumerate}
Steps 2 to 5 are repeated while $g$ < $n$.

Processing packages as a chunk allows getting a summary of the information sent in a $w$ period of time, such as: total bytes sent, total bytes sent through TCP and UDP protocol, total bytes sent for web traffic destination along 80, 443 and 3128 ports and as we are working inside and Campus Area Network total bytes sent to internal destination (same backbone ip, our case 148.201.X.X). This data is condensed into three metrics: a) TCP-UDP metric, represents the ratio between total bytes sent through both protocols and total bytes sent in the chunk , b) Internal IP metric, represents the ratio between total bytes sent to CAN proxy ip and total bytes sent in the chunk and c) Web traffic metric, represents the ratio between data sent through web ports, and and total bytes sent in the chunk.
Obtained metrics are the $k$-values of the neurons in the Self Organizing Map lattice.




\paragraph{Self Organizing Map algorithm implementation} % level 4
Self Organizing Map basic algorithm is defined as follows.
\begin{enumerate}
\item Initialization. Choose random elements for the initial weight vectors $w_j$.
\item Sampling. Select a sample training input vector $x_n$ from the input space.
\item Matching. Find the winning neuron $L(x)$, that has it's weight vector closest to the input vector $x$.
\item Updating. Apply the weight update equation $\Delta w_j=\eta(t) \times \kappa(t) \times (x_i - w_j)$; where $\eta(t)$ is a gaussian neighborhood function and $\kappa(t)$ is the learning rate.
\item Continuation. Keep returning to step 2 until the feature map stops changing.
\end{enumerate}

Each element in the SOM lattice has a weights vector $V(v_1, v_2,..., v_n)$ of three elements, $v_1$ represents the TCP/UDP metric, $v_2$ represents the Internal IP metric and $v_3$ represents the Web traffic metric.

To define the the size $n$ of the lattice, experiments were done with 50, 75, 100 and 125 elements. Table 1. shows the time needed to train the lattices. As it can be seen as more elements in the lattice, time increases, but the results when comparing the obtained patterns against other users are very similar.The lattices used for the experiments were build using XX elements,  obtained randomly from the Build data set $\beta_1$ and ordered in a square distribution.

For the evaluation phase, a new set based on the Training data set $\gamma_1$ was obtained. The average number of elements on the train data set of each user was 56,496 elements, the created data set, was conformed by 100,000 elements, guaranteeing that each element was evaluated at least one time.

Best matching unit evaluation was obtained using Euclidean distance. The gaussian neighborhood function was used with an initial neighborhood radius of ($\frac{n}{2}$), decreasing linearly to 1 at the end of the training. The learning rate was chosen to be 1 and reduced to 0 at the end of the training.




\paragraph{User network behavior pattern creation} % level 4
The train module implements the SOM-based approach for user characterization. In the training phase 10 epochs are used to process the lattice. After this phase, an user network behavior pattern is obtained. This pattern may differ between patterns obtained from the same user, because of the random selection of the build elements. Fig 3 shows lattice training during it's initial phases until it get's completely trained.

\begin{center}\includegraphics[scale=0.2]{fig-training} \end{center}





\subsubsection{Pattern evaluation} % level 3
When SOM lattice is completely trained, the elements with the same characteristics in their weights vector are grouped in the same area, denoting clusters of elements and a inherent classification. This obtained distribution of the elements along the SOM lattice, allows to classify easily new vectors, even if them were not in the the build or the train dataset [Reference, XX].

The pattern evaluation module use the capacity of the SOM lattice, of matching new input vectors to their similar ones, not for classifying a new element, but for deciding if an user behavior can be identified among others. To perform this evaluation, new elements are needed: a) an organizational pattern and b) an organization evaluation dataset. An organizational pattern is defined as a matrix of user patterns $P$, which represent an user with it's own behavior in an environment where different users with common or completely different behaviors are present. The organization evaluation pattern is a collection of evaluation data sets $\phi_1$ of the users that conform the organizational pattern, acting as a labeled evaluation data set.

Evaluation is done by showing each element of the organization evaluation data set to the organizational pattern, resulting best matching units are processed by obtaining the user pattern they belong to and compare it against the expected result. Fig 4. shows how an organizational pattern is conformed and how it is evaluated.

	\begin{center}\includegraphics[scale=0.3]{evaluation} \end{center}

\section{Results and Discussion} % level 1
Results presentation, how the results are interpreted, and what we can do with data.
The results will explain, how the user is able to recognize itself in the organization map.






\section{Conclusions} % level 1

\subsection{Future work} % level 2
Due hardware limitation, SOM training is done with ten epochs. A much longer training of about one thousand epochs would give a more precise user pattern, helping in a better user detection in the organization map.
Also formulas are not completely following the standard of a gaussian function so a new implementation would be great.






\section{Bibliography} % level 1
[1] Ramadas, M., Ostermann, S.,  Tjaden, B. Detecting Anomalous Network Traffic with Self-organizing Maps.

[2] ?OSI? http://www.webopedia.com/TERM/O/OSI.html.

[3] Baker, David W. ?Communications and Networking? Using Java 1.1, Third Edition

[4] http://searchnetworking.techtarget.com/definition/Transport-layer

[5] https://www.rfc-es.org/rfc/rfc0793-es.txt

[6] https://www.ietf.org/rfc/rfc768.txt

[7] Zimmermann, Hubert. 1990. «OSI Reference Model - The ISO Model of Architecture for Open Systems Interconnection.» IEEE Transactions 425-432.

[8] Dozono, H., Itou, S., and Nakakuni, M. (2007). Comparison of the adaptive authentication systems for behavior biometrics using the variations of self organizing maps. International 
Journal of Computers and Communications, 1(4), 108-116.

[9]Kurose, James, y Keith Ross. 2012. Computer Networking: a top down approach. New
Jersey: Pearon.

[10] CISCO, "Annual Security Report," 2015.

[11] L. Zhe, S. Weiqing and W. Lingfeng, "A neural network based distributed intrusion detection system on cloud platform," IEEE 2nd International Conference on Cloud Computing and Intelligence Systems, vol. 1, pp. 75-79, 2012. 

[25] T.Kohonen. Self Organizing Maps. Springer, third edition, 2001.





[2] K. A. Jalil and N. M. Masrek, "Comparison of Machine Learning Algorithms Performance in Detecting Network," 2010 International Conference on Networking and Information Technology , p. 6, 2010. 

[3] Al-Jarrah, Siddiqui, Elsalamouny, Yoo, Muhaidat and Kim, "Machine-Learning-Based Feature Selection Techniques for LargeScale," 2014 IEEE 34th International Conference on Distributed Computing Systems Workshops, p. 5, 2014. 

[4] B. Chaudhari and R. Prasad, "Particle Swarm Optimization Based Intrusion Detection for Mobile," 9th International Conference on Computer Engineering and Applications, p. 6, 2015. 

[5] CISCO, "Annual Security Report," 2015.



[7] K. Al-Jarrah, "Machine-learning-based feature selection techniques for large-scale network intrusion detection," IEEE 34th International Conference on Distributed Computing Systems Workshops (ICDCSW), pp. 177-181, 2014. 

[8] S. Pilabutr, "Integrated soft computing for Intrusion Detection on computer network security," Computer Applications and Industrial Electronics (ICCAIE), 2011 IEEE International Conference, pp. 559-563, 2011. 

[9] W.L. J. Tian, A new network intrusion detection identification model research., Informatics in Control, Automation and Robotics (CAR), 2010 2nd International Asia Conference, vol. 2, pp. 9-12, 2010. 

[10] T. Yamada, Intrusion detection system to detect variant attacks using learning algorithms with automatic generation of training data, International Conference on Information Technology: Coding and Computing (ITCC'05)-Volume II, vol. 1, pp. 650-655, 2005. 

[11] S. Chaudhari, Particle Swarm Optimization Based Intrusion Detection for Mobile Ad-hoc Networks










\end{document}